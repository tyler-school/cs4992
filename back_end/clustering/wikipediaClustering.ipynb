{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "contentList = []\n",
    "matchList = []\n",
    "\n",
    "#Number of clusters\n",
    "true_k = 4\n",
    "\n",
    "\"\"\"\n",
    "This program is meant to perform clustering on a list of articles, currently called contentList.\n",
    "Given this list of text, it will cluster them. This creates three sets of important data (might be more to be retrived if wanted)\n",
    " - Number of articles in each cluster \n",
    " - Words that are associated with a specific cluster\n",
    " - Articles that are in a cluster (This can be replaced with an ID to potentially make processing easier)\n",
    "\n",
    "For the time being, this information is just printed out to the terminal. \n",
    "It could instead easily be packaged into a JSON to send to front-end. \n",
    "\n",
    "Issues:\n",
    "The clustering program was tested with a a significant portion of wikipedia. \n",
    "\n",
    "The words which are associated with a cluster may not be the best topic names. \n",
    "For example, genus, family, species may show up for science/biology themed articles.\n",
    "Could pre-identify these names to prettier topics, as these are very common and predictable. \n",
    "\n",
    "Clustering requires that a number of clusters is provided, prior to calculating them.\n",
    "The number of clusters is important in regards to the accuracy of clustering, so the correct number must be chosen.\n",
    "\n",
    "Traditionally, this is done by performing clustering with a range of cluster sizes, \n",
    "and performing an analysis to determine a knee/bend in the data slope-wise. \n",
    "This can be quite slow for larger data sets, and for some data sets no distinguishable knee exists. \n",
    "\n",
    "Putting an arbitrary cluster size can result in too little or too few clusters. \n",
    "Probably want to take a look at this during implementation, shouldn't be too hard to fix using sampling.\n",
    "\n",
    "Other than that, code still has some extra stuff from tutorials in terms of benchmarking performance, and the function names and layout kind of sucks\n",
    "\"\"\"\n",
    "\n",
    "#Fetches data that may be useful for testing the model\n",
    "def getData():\n",
    "    for x in range(1, 10):\n",
    "        opener = open('./wikipediaFolder/' + str(x) + '.ext', 'r')\n",
    "        fcc_data = json.load(opener)\n",
    "        for i in fcc_data:\n",
    "            contentList.append(i[\"text\"])\n",
    "\n",
    "    for x in range(0, len(contentList)):\n",
    "        matchList.append(x)\n",
    "\n",
    "getData()\n",
    "\n",
    "#performs the fit a few times, using random seeds\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=1):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        km.fit(X)\n",
    "\n",
    "\n",
    "def run_model():\n",
    "    #Creates a vectorizer, setting guidelines as to how text should be processed/reduced\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=0.5,\n",
    "        min_df=5,\n",
    "        stop_words=\"english\",\n",
    "    )\n",
    "\n",
    "    #Performs the actual vectorization on the list of articles, using lsa for speed improvements\n",
    "    t0 = time()\n",
    "    X_tfidf = vectorizer.fit_transform(contentList)\n",
    "    lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\n",
    "    t0 = time()\n",
    "    X_lsa = lsa.fit_transform(X_tfidf)\n",
    "    explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "    print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "    print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\n",
    "\n",
    "\n",
    "\n",
    "    #Given the vectorized content, we can go about running the model\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=true_k,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "    )\n",
    "    fit_and_evaluate(kmeans, X_lsa, name=\"KMeans\\nwith LSA on tf-idf vectors\")\n",
    "\n",
    "    #Fetch the terms associated with a cluster. This can become a cluster name.\n",
    "    original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    for i in range(true_k):\n",
    "        print(f\"Cluster {i}: \", end=\"\")\n",
    "        for ind in order_centroids[i, :20]:\n",
    "            print(f\"{terms[ind]} \", end=\"\")\n",
    "        print()\n",
    "\n",
    "\n",
    "    #Get what cluster a specific article is in, and print it out. \n",
    "    labels=kmeans.labels_\n",
    "    clusters=pd.DataFrame(list(zip(contentList,labels)),columns=['title','cluster'])\n",
    "    print(clusters.sort_values(by=['cluster']))\n",
    "\n",
    "    for i in range(true_k):\n",
    "        print(clusters[clusters['cluster'] == i])\n",
    "\n",
    "run_model()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32f7d14dbbb81a0c39b3a3c11fd885ebadee09558b61cd74af3f1f8f75f60e6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
